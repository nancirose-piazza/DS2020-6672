{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "midterm-nanci.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3o9G2DCFzp7",
        "colab_type": "text"
      },
      "source": [
        "This notebook is the preprocessing and general format of the architecture for the deep neural network. The midterm-train notebook is for executing the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gah4UHkTyQSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuEiHFmTyQSl",
        "colab_type": "text"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTZeCD-YyQSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing tensorflow\n",
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9poc6bPyQSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmInL3h7yQS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DknxlJKgyQTE",
        "colab_type": "text"
      },
      "source": [
        "# Loading From Link and Saving to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isuQh6efEzwZ",
        "colab_type": "code",
        "outputId": "a51be89c-fa51-4e2c-cc7e-92da13e51985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "!wget https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 18:58:32--  https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2\n",
            "Resolving pubdata.endgame.com (pubdata.endgame.com)... 64.250.189.21\n",
            "Connecting to pubdata.endgame.com (pubdata.endgame.com)|64.250.189.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1751237573 (1.6G) [application/octet-stream]\n",
            "Saving to: ‘ember_dataset_2017_2.tar.bz2’\n",
            "\n",
            "ember_dataset_2017_ 100%[===================>]   1.63G  14.2MB/s    in 2m 39s  \n",
            "\n",
            "2020-04-03 19:01:13 (10.5 MB/s) - ‘ember_dataset_2017_2.tar.bz2’ saved [1751237573/1751237573]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThN4KybFKyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bzip2 -d ember_dataset_2017_2.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2td_MRFU7a",
        "colab_type": "code",
        "outputId": "c020aa9b-82bd-47b3-971a-9fe7994ec137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!tar -xvf ember_dataset_2017_2.tar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ember_2017_2/\n",
            "ember_2017_2/train_features_1.jsonl\n",
            "ember_2017_2/train_features_0.jsonl\n",
            "ember_2017_2/train_features_3.jsonl\n",
            "ember_2017_2/test_features.jsonl\n",
            "ember_2017_2/train_features_5.jsonl\n",
            "ember_2017_2/train_features_4.jsonl\n",
            "ember_2017_2/train_features_2.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY90MRaUFi-w",
        "colab_type": "code",
        "outputId": "1b7b50b8-8fb2-4011-9469-4d528b5d1b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!rm ember_dataset_2017_2.tar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'ember_dataset_2017_2.tar': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njc40X6Oaa-d",
        "colab_type": "code",
        "outputId": "f5019c90-8f7a-4bc6-d3f1-55b40855832d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        }
      },
      "source": [
        "!wget https://github.com/endgameinc/ember/archive/master.zip\n",
        "!unzip master.zip\n",
        "!rm master.zip\n",
        "!cp -r ember-master/* .\n",
        "!rm -r ember-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-05 14:44:29--  https://github.com/endgameinc/ember/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/endgameinc/ember/zip/master [following]\n",
            "--2020-04-05 14:44:29--  https://codeload.github.com/endgameinc/ember/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [  <=>               ]  11.22M  13.8MB/s    in 0.8s    \n",
            "\n",
            "2020-04-05 14:44:30 (13.8 MB/s) - ‘master.zip’ saved [11769023]\n",
            "\n",
            "Archive:  master.zip\n",
            "04c37efb20ebeb7b99d8ec1d5cdd20ab0328bb36\n",
            "   creating: ember-master/\n",
            "  inflating: ember-master/LICENSE.txt  \n",
            "  inflating: ember-master/README.md  \n",
            "   creating: ember-master/ember/\n",
            "  inflating: ember-master/ember/__init__.py  \n",
            "  inflating: ember-master/ember/features.py  \n",
            "   creating: ember-master/licenses/\n",
            "  inflating: ember-master/licenses/AGPL-LICENSE-3.0.txt  \n",
            "  inflating: ember-master/licenses/MIT-LICENSE.txt  \n",
            "   creating: ember-master/malconv/\n",
            "  inflating: ember-master/malconv/README.md  \n",
            "  inflating: ember-master/malconv/malconv.h5  \n",
            "  inflating: ember-master/malconv/malconv.py  \n",
            "  inflating: ember-master/malconv/multi_gpu.py  \n",
            "  inflating: ember-master/requirements.txt  \n",
            "  inflating: ember-master/requirements_conda.txt  \n",
            "  inflating: ember-master/requirements_notebook.txt  \n",
            "   creating: ember-master/resources/\n",
            "  inflating: ember-master/resources/ember-notebook.ipynb  \n",
            "  inflating: ember-master/resources/ember2018-notebook.ipynb  \n",
            "  inflating: ember-master/resources/logo.png  \n",
            "   creating: ember-master/scripts/\n",
            "  inflating: ember-master/scripts/classify_binaries.py  \n",
            "  inflating: ember-master/scripts/train_ember.py  \n",
            "  inflating: ember-master/setup.py   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsm-JuY9am2N",
        "colab_type": "code",
        "outputId": "4ac6191f-6f24-4529-e55f-43c674d2fe9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lief==0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/0e/0d6f3357975dd1530aeb4b4a84a99d493775391758378fb5109f47b613f9/lief-0.9.0.zip\n",
            "Requirement already satisfied: tqdm>=4.31.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.38.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.0.3)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->-r requirements.txt (line 4)) (1.12.0)\n",
            "Building wheels for collected packages: lief\n",
            "  Building wheel for lief (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lief: filename=lief-0.9.0-cp36-cp36m-linux_x86_64.whl size=3610316 sha256=63a57d72e6407f31638bbe7a16e410bc9f81a0191542d7228a8f43afd4c2b073\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/7d/da/deca5cc6d3c5f0d2612e55c691b7c2e57087df18de92733bff\n",
            "Successfully built lief\n",
            "Installing collected packages: lief\n",
            "Successfully installed lief-0.9.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating ember.egg-info\n",
            "writing ember.egg-info/PKG-INFO\n",
            "writing dependency_links to ember.egg-info/dependency_links.txt\n",
            "writing requirements to ember.egg-info/requires.txt\n",
            "writing top-level names to ember.egg-info/top_level.txt\n",
            "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
            "reading manifest file 'ember.egg-info/SOURCES.txt'\n",
            "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/ember\n",
            "copying ember/__init__.py -> build/lib/ember\n",
            "copying ember/features.py -> build/lib/ember\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/__init__.py -> build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/features.py -> build/bdist.linux-x86_64/egg/ember\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/features.py to features.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/ember-0.1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ember-0.1.0-py3.6.egg\n",
            "Copying ember-0.1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding ember 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/ember-0.1.0-py3.6.egg\n",
            "Processing dependencies for ember==0.1.0\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for lightgbm==2.2.3\n",
            "Best match: lightgbm 2.2.3\n",
            "Adding lightgbm 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pandas==1.0.3\n",
            "Best match: pandas 1.0.3\n",
            "Adding pandas 1.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.2\n",
            "Best match: numpy 1.18.2\n",
            "Adding numpy 1.18.2 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tqdm==4.38.0\n",
            "Best match: tqdm 4.38.0\n",
            "Adding tqdm 4.38.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for lief==0.9.0\n",
            "Best match: lief 0.9.0\n",
            "Adding lief 0.9.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for joblib==0.14.1\n",
            "Best match: joblib 0.14.1\n",
            "Adding joblib 0.14.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for ember==0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21AgLvOuarKl",
        "colab_type": "code",
        "outputId": "503a9889-6a60-4d2e-c06f-95e8782cb2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "import ember\n",
        "ember.create_vectorized_features(\"ember_2017_2/\")\n",
        "ember.create_metadata(\"ember_2017_2/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizing training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 900000/900000 [12:23<00:00, 1211.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vectorizing test set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200000/200000 [02:45<00:00, 1209.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sha256</th>\n",
              "      <th>appeared</th>\n",
              "      <th>subset</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n",
              "      <td>2006-12</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n",
              "      <td>2006-12</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n",
              "      <td>2007-01</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n",
              "      <td>2007-02</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n",
              "      <td>2007-02</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099995</th>\n",
              "      <td>fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...</td>\n",
              "      <td>2017-12</td>\n",
              "      <td>test</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099996</th>\n",
              "      <td>fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...</td>\n",
              "      <td>2017-12</td>\n",
              "      <td>test</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099997</th>\n",
              "      <td>fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...</td>\n",
              "      <td>2017-12</td>\n",
              "      <td>test</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099998</th>\n",
              "      <td>fffe92f9593649c4a7050302368189de45e2c1c06b04ea...</td>\n",
              "      <td>2017-12</td>\n",
              "      <td>test</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099999</th>\n",
              "      <td>ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...</td>\n",
              "      <td>2017-12</td>\n",
              "      <td>test</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    sha256  ... label\n",
              "0        0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  ...     0\n",
              "1        d4206650743b3d519106dea10a38a55c30467c3d9f7875...  ...     0\n",
              "2        c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  ...     0\n",
              "3        7f513818bcc276c531af2e641c597744da807e21cc1160...  ...     0\n",
              "4        ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  ...     0\n",
              "...                                                    ...  ...   ...\n",
              "1099995  fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...  ...     0\n",
              "1099996  fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...  ...     1\n",
              "1099997  fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...  ...     0\n",
              "1099998  fffe92f9593649c4a7050302368189de45e2c1c06b04ea...  ...     1\n",
              "1099999  ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...  ...     1\n",
              "\n",
              "[1100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nytbMk0odX4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cp ember_2017_2/X_train.dat drive/My\\ Drive/data/ember_2017_2/\n",
        "#!cp ember_2017_2/y_train.dat drive/My\\ Drive/data/ember_2017_2/\n",
        "!cp ember_2017_2/X_test.dat drive/My\\ Drive/data/ember_2017_2/\n",
        "!cp ember_2017_2/y_test.dat drive/My\\ Drive/data/ember_2017_2/\n",
        "!cp ember_2017_2/metadata.csv drive/My\\ Drive/data/ember_2017_2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sw4K7V-hwPs",
        "colab_type": "text"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qX9K4Ai6d1nM",
        "colab": {}
      },
      "source": [
        "#NEED MORE RAMMM\n",
        "a = []\n",
        "while(1):\n",
        "    a.append(\"1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mU4ZA3yQU-",
        "colab_type": "text"
      },
      "source": [
        "# Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m02QptdKa01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model():\n",
        "  import tensorflow as tf\n",
        "  from tensorflow import keras\n",
        "  from tensorflow.keras import layers\n",
        "  feature_size=2381\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  #Model architecture\n",
        "  from tensorflow.keras import layers\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.InputLayer(input_shape=(1,feature_size)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(1500, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision()])\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R28fXRPXpqix",
        "colab_type": "code",
        "outputId": "d8aa4712-0c22-4482-c80f-02fb7b186a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout (Dropout)            (None, 1, 2381)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1, 1500)           3573000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1500)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 1)              1501      \n",
            "=================================================================\n",
            "Total params: 3,574,501\n",
            "Trainable params: 3,574,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0NbaX3rVivv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"drive/My Drive/data/pickles/\"+'my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArjnH4jzy6sv",
        "colab_type": "text"
      },
      "source": [
        "# Standardizing Data and Taking Relevent Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4z0UiAvKurJ",
        "colab_type": "text"
      },
      "source": [
        "Since the library lief has done the heavy lifting in terms of vectorizing the features, actually--  I spent like 6+ hours doing that myself to come to figure out I didn't need to do it... We must normalize the data values. Since the data is too large to load into memory and fit to the scaler in the same runtime execution, we can prep the data in steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaECqG9ZLTpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"ember_2017_2/\")\n",
        "metadata_dataframe = ember.read_metadata(\"ember_2017_2/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjLGCdB5LaQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelrows = (y_train != -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEqTXUSALcSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owEOkpzwLe3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this to make a h5 file in the session in case the ram crashes\n",
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPJh6QBnLhTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjdrMdu0LruP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read to avoid having to filter the data for labeled samples again\n",
        "import h5py\n",
        "h5f = h5py.File('X_train.h5','r')\n",
        "X_train = h5f['X_train'][:]\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvk-ZuiYL2EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#by checking the size of y_train, we find that there's 600k labels. Cool, we can partial fit the standardScaler to avoid overloading the memory\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "\n",
        "for x in range(0,600000,100000):\n",
        "  mms.partial_fit(X_train[x:x+100000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym9nfJi1L2G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#do this in case the ram crashes...\n",
        "with open(\"mms_scaler\",\"wb\") as f:\n",
        "  import pickle\n",
        "  pickle.dump(mms,f)\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUii2H2aL2Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transform the data\n",
        "X_train = mms.transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqd-HDjc1ZH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape data to have 3 channels...\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baHbs_i2utiD",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIHRJB4LGY5Y",
        "colab_type": "text"
      },
      "source": [
        "Training is done in a separate notebook namely because there was lots of testing hyperparameters. However, here is the execution code minus any outputs. The block of code was executed separately several times with 1 epoch set to have saved models in case it overshoot the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j64uLJ_kOHaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "pkl_base=\"drive/My Drive/data/pickles/\"\n",
        "\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision()])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                batch_size=256,\n",
        "                epochs=1,\n",
        "                  validation_data =((X_train[600000-120000:600000],y_train[600000-120000:600000]))\n",
        "                  )\n",
        "model_name=\"my_model2.h5\"\n",
        "model_weights=\"weights2.h5\"\n",
        "model.save(pkl_base+model_name)\n",
        "model.save_weights(pkl_base+model_weights)\n",
        "print(model_name,model_weights,\" are saved.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI_3QCb9GfCp",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0isIMAN5ssLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read to avoid having to filter the data for labeled samples again\n",
        "import h5py\n",
        "h5f = h5py.File('drive/My Drive/data/ember_2017_2/X_test.h5','r')\n",
        "X_test = h5f['X_test'][:]\n",
        "h5f.close()\n",
        "#read to avoid having to filter the data for labeled samples again\n",
        "import h5py\n",
        "h5f = h5py.File('drive/My Drive/data/ember_2017_2/y_test.h5','r')\n",
        "y_test = h5f['y_test'][:]\n",
        "h5f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNknd57Fs4fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting testing data too\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "\n",
        "X_test = mms.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VEJ-6NAX9J25",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "X_test = np.reshape(X_test,(-1,1,2381))\n",
        "y_test = np.reshape(y_test,(-1,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--nOWRR3uS2o",
        "colab_type": "code",
        "outputId": "96586112-814b-4d6d-87aa-8b60db643656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "model = tf.keras.models.load_model(\"drive/My Drive/data/pickles/\"+'my_model_test-tpu3.h5')\n",
        "\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Precision()])\n",
        "\n",
        "results =model.evaluate(X_test,y_test)\n",
        "print(\"loss: %gl,acc: %gl\"%(results[0],results[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 5.09298l,acc: 0.97538l\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvfz3REhsGu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJhQXhTsshs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.reshape(y_test,(-1))\n",
        "pred = np.reshape(pred,(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvwDZZLpswCM",
        "colab_type": "code",
        "outputId": "b27bab35-415f-4e32-95a1-5032a432d86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "y_test,pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1., 1., 0., ..., 0., 1., 1.], dtype=float32),\n",
              " array([1., 1., 0., ..., 0., 1., 1.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVdaI13etd0S",
        "colab_type": "code",
        "outputId": "05d572ab-ce37-4334-9f0d-3682cf867053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "y_test.shape,pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((200000,), (200000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfehWTxvr2M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "fpr, tpr, tresh = metrics.roc_curve(y_test,pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmrqc2dt6Lo",
        "colab_type": "code",
        "outputId": "cd34431a-8904-4cf2-b255-9e23ae05ed43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "metrics.auc(fpr,tpr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9853060999000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qFwKy32u4bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_int = np.asarray(y_test,dtype=int)\n",
        "pred_int=np.asarray(pred,dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH74vbSkuqIn",
        "colab_type": "code",
        "outputId": "4c4b9f0e-8b17-4699-c096-40f8acf47d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "fscore = f1_score(y_test_int, pred_int) \n",
        "fscore"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9724353081247269"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZupBim7BCEwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRfFWn2nBuJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3fab27ab-a5f8-4b8b-b46d-295057cbb5ba"
      },
      "source": [
        "precision = sklearn.metrics.precision_score(y_test_int,pred_int)\n",
        "precision"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9770540788822822"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZsOWaXVvWCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test_int,pred_int,labels=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDT6Hbnjvx7Z",
        "colab_type": "code",
        "outputId": "5c664625-803c-479f-8c01-af09be413fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(cm,columns=['pred_benign','pred_malicious'],index=['is_benign','is_malicious'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred_benign</th>\n",
              "      <th>pred_malicious</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>is_benign</th>\n",
              "      <td>97727</td>\n",
              "      <td>2273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_malicious</th>\n",
              "      <td>3214</td>\n",
              "      <td>96786</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              pred_benign  pred_malicious\n",
              "is_benign           97727            2273\n",
              "is_malicious         3214           96786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmgOzZVK0ni6",
        "colab_type": "code",
        "outputId": "5305d7b3-9cf7-45d0-82ce-61176e911b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "!wget https://the.earth.li/~sgtatham/putty/latest/w32/putty.exe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-05 14:50:44--  https://the.earth.li/~sgtatham/putty/latest/w32/putty.exe\n",
            "Resolving the.earth.li (the.earth.li)... 93.93.131.124, 2a00:1098:86:4d:c0ff:ee:15:900d\n",
            "Connecting to the.earth.li (the.earth.li)|93.93.131.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://the.earth.li/~sgtatham/putty/0.73/w32/putty.exe [following]\n",
            "--2020-04-05 14:50:45--  https://the.earth.li/~sgtatham/putty/0.73/w32/putty.exe\n",
            "Reusing existing connection to the.earth.li:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1096080 (1.0M) [application/x-msdos-program]\n",
            "Saving to: ‘putty.exe’\n",
            "\n",
            "putty.exe           100%[===================>]   1.04M  1.27MB/s    in 0.8s    \n",
            "\n",
            "2020-04-05 14:50:46 (1.27 MB/s) - ‘putty.exe’ saved [1096080/1096080]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6taiQfT2TyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/My\\ Drive/data/pickles/mms_scaler_test ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGE4nnCl3Qi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/My\\ Drive/data/pickles/my_model_test-tpu3.h5 model.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CUbPxMdzBNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_pefile(pefile):\n",
        "  try:\n",
        "    import ember\n",
        "  except:\n",
        "    !wget https://github.com/endgameinc/ember/archive/master.zip\n",
        "    !unzip master.zip\n",
        "    !rm master.zip\n",
        "    !cp -r ember-master/* .\n",
        "    !rm -r ember-master\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py install\n",
        "    import ember\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  import tensorflow as tf\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "  with open(\"mms_scaler_test\",\"rb\") as f:\n",
        "    import pickle\n",
        "    mms = pickle.load(f)\n",
        "    f.close()\n",
        "  \n",
        "  sample_data = open(pefile, \"rb\").read()\n",
        "  extractor = ember.PEFeatureExtractor(2)\n",
        "  sample_data = np.array(extractor.feature_vector(sample_data), dtype=np.float32)\n",
        "  sample_data = mms.transform([sample_data])\n",
        "  sample_data = np.reshape(sample_data,(-1,1,2381))\n",
        "\n",
        "  model = tf.keras.models.load_model('model.h5')\n",
        "  pred = model.predict_classes(sample_data)\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrazXMiI0Nr4",
        "colab_type": "code",
        "outputId": "1e4eec30-dcc7-4900-d025-0def59e9e4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "test_pefile(\"putty.exe\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-fd0bf5dddfde>:19: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0]]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}